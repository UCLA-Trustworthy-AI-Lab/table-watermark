{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "##### adding watermark #####\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def getGreenList(lo=0, hi=1, m=1000):\n",
    "    \"\"\" return a list of tuple, representing the green list\n",
    "    \"\"\"\n",
    "\n",
    "    waymarks = np.linspace(lo, hi, m + 1)\n",
    "\n",
    "    green_list = []\n",
    "    for i in range(0, m, 2):\n",
    "        # select $delta_i^{2k}$ in equ.(2) with prob.\n",
    "        if np.random.uniform() > .5:\n",
    "            i += 1\n",
    "        green_list.append([waymarks[i], waymarks[i + 1]])\n",
    "\n",
    "    return green_list\n",
    "\n",
    "def singleColumnWatermark(arr, GreenListsInfo, col_id, m):\n",
    "    \"\"\" add watermark to a column based on a green list, normalized by the\n",
    "        empirical statistics of a generative model\n",
    "    \"\"\"\n",
    "    arr_wm = arr.copy()\n",
    "    mean = GreenListsInfo['col_mean'][col_id]\n",
    "    std = GreenListsInfo['col_std'][col_id]\n",
    "    green_list = GreenListsInfo['GreenLists'][col_id]\n",
    "    arr_wm = (arr_wm - mean) / std\n",
    "    \n",
    "    for i in range(len(arr_wm)):\n",
    "        # offset elem to [0, 1]\n",
    "        e_flr = np.floor(arr_wm[i])\n",
    "        e = arr_wm[i] - e_flr\n",
    "\n",
    "        g = findNearestIntervalHash(e, green_list, m)\n",
    "\n",
    "        if e > g[1] or e < g[0]:\n",
    "            # if x[i] falls outside of the range, then\n",
    "            # we re-sample the elem. from a uniform dist.\n",
    "            e_wm = np.random.uniform(g[0], g[1]) \n",
    "            arr_wm[i] = e_wm + e_flr\n",
    "\n",
    "        else:\n",
    "            e_wm = e\n",
    "    \n",
    "        # print(\"elem: {}, int: {}, {}, elem_wm: {}, ofst: {}\".format(e, g[0], g[1], e_wm, e_flr))\n",
    "    arr_wm = arr_wm * std + mean\n",
    "\n",
    "    return arr_wm, mean, std\n",
    "\n",
    "def findNearestInterval(e, green_list):\n",
    "    \"\"\" return the nearest interval to the given element e\n",
    "    \"\"\"\n",
    "\n",
    "    min_dist, min_indx = np.inf, -1\n",
    "\n",
    "    # offset elem to [0, 1]\n",
    "    e = e - np.floor(e)\n",
    "\n",
    "    for i, intv in enumerate(green_list):\n",
    "        cur_dist = np.abs(e - (intv[0] + intv[1]) / 2)\n",
    "\n",
    "        if cur_dist < min_dist:\n",
    "            min_dist = cur_dist\n",
    "            min_indx = i\n",
    "\n",
    "    return green_list[min_indx]\n",
    "\n",
    "def findNearestIntervalHash(e, green_list, m):\n",
    "    \"\"\" return the nearest interval to the given element e\n",
    "    \"\"\"\n",
    "\n",
    "    min_dist, min_indx = np.inf, -1\n",
    "\n",
    "    # offset elem to [0, 1]\n",
    "    e = e - np.floor(e)\n",
    "\n",
    "    # hashing\n",
    "    idx_c = int(e // (2 / m))\n",
    "    idx_l0, idx_r0 = max(0, idx_c - 1), min(idx_c + 1, len(green_list) - 1)\n",
    "    idx_l1, idx_r1 = max(0, idx_c - 2), min(idx_c + 2, len(green_list) - 1)\n",
    "\n",
    "    local_g_list = [green_list[idx_l1], green_list[idx_l0], green_list[idx_c], green_list[idx_r0], green_list[idx_r1]]\n",
    "    for i, intv in enumerate(local_g_list):\n",
    "        cur_dist = np.abs(e - (intv[0] + intv[1]) / 2)\n",
    "\n",
    "        if cur_dist < min_dist:\n",
    "            min_dist = cur_dist\n",
    "            min_indx = i\n",
    "\n",
    "    return local_g_list[min_indx]\n",
    "\n",
    "def countElemInGreenList(arr, green_list, m):\n",
    "    \"\"\" Count elements in the array falling within the green list intervals.\n",
    "    \"\"\"\n",
    "    T = 0\n",
    "\n",
    "    for e in arr:\n",
    "        g = findNearestIntervalHash(e, green_list, m)\n",
    "\n",
    "        e_ofst = e - np.floor(e)\n",
    "\n",
    "        T += (e_ofst <= g[1] and e_ofst >= g[0])\n",
    "        # print(g[0], g[1], e, e_ofst)\n",
    "\n",
    "    return T\n",
    "\n",
    "def binomTesting(arr, green_list, m):\n",
    "    T = countElemInGreenList(arr, green_list, m)\n",
    "    p = binom.cdf(T, len(arr), 0.5)\n",
    "    print(T)\n",
    "    return 1 - p\n",
    "\n",
    "def gaussTesting(arr, green_list, m):\n",
    "    T = countElemInGreenList(arr, green_list, m)\n",
    "\n",
    "    mv = len(arr) / 2\n",
    "    std = np.sqrt(len(arr)) / 2\n",
    "    p = norm.cdf((T - mv) / std)\n",
    "    print(T)\n",
    "    return 1 - p    \n",
    "\n",
    "def gaussMultTesting(arrs, green_lists, m):\n",
    "    \"\"\" The proposed method which performs hypothesis testing across multiple arrays with chi-squared distribution.\n",
    "    \"\"\"\n",
    "    X = 0\n",
    "\n",
    "    mv = len(arrs[0]) / 2\n",
    "    std = np.sqrt(len(arrs[0])) / 2\n",
    "\n",
    "    for arr, green_list in zip(arrs, green_lists):\n",
    "        T = countElemInGreenList(arr, green_list, m)\n",
    "        #print(T)\n",
    "\n",
    "        T_c = (T - mv) / std\n",
    "        X += T_c ** 2\n",
    "\n",
    "    p = chi2.cdf(X, len(arrs))\n",
    "    return 1 - p\n",
    "\n",
    "def exactMaxGaussCDF(x, n):\n",
    "    return norm.cdf(x)**n\n",
    "\n",
    "def maxGaussTesting(arrs, green_lists, m):\n",
    "    \"\"\" an optional hypothesis testing method with extreme statistics\n",
    "    \"\"\"\n",
    "    max_Tc = float('-inf')\n",
    "\n",
    "    mv = len(arrs[0]) / 2\n",
    "    std = np.sqrt(len(arrs[0])) / 2\n",
    "\n",
    "    for arr, green_list in zip(arrs, green_lists):\n",
    "        T = countElemInGreenList(arr, green_list, m)\n",
    "        T_c = (T - mv) / std\n",
    "        if T_c > max_Tc:\n",
    "            max_Tc = T_c\n",
    "\n",
    "    p = exactMaxGaussCDF(max_Tc, len(arrs))\n",
    "    return 1 - p\n",
    "\n",
    "def genMixtureGaussData(n, k=5):\n",
    "    mv = np.random.randn(1, k) + np.random.randn(n, k)\n",
    "    ma = np.eye(k)[np.random.choice(k, n)]\n",
    "    return np.sum(mv * ma, axis=1)\n",
    "\n",
    "def simulateData(n=10, m=1000):\n",
    "    tab_list_wm = [np.random.randn(n) for __ in range(5)]\n",
    "    tab_list_nwm = [np.random.randn(n) for __ in range(5)]\n",
    "    # create green list\n",
    "    g_list = getGreenList(m=m)\n",
    "\n",
    "    print(\"############# n = {} #############\".format(n))\n",
    "    for tab in tab_list_nwm:\n",
    "        print(\"Binom: {:.8f}, Gauss: {:.8f}\".format(binomTesting(tab, g_list, m), gaussTesting(tab, g_list, m)))\n",
    "\n",
    "    for tab in tab_list_wm:\n",
    "        tab_wm = singleColumnWatermark(tab, g_list, m)\n",
    "        print(\"Binom: {:.8f}, Gauss: {:.8f}\".format(binomTesting(tab_wm, g_list, m), gaussTesting(tab_wm, g_list, m)))\n",
    "\n",
    "def simulateMultiColData(n=10, c=10, m=1000):\n",
    "    # create green list\n",
    "    g_lists = [getGreenList(m=m) for __ in range(c)]\n",
    "\n",
    "    tab_lists_no_wm = [np.random.randn(n) for __ in range(c)]\n",
    "    tab_lists_wm_sing = [np.random.randn(n) for __ in range(c)]\n",
    "    tab_lists_wm_all = [np.random.randn(n) for __ in range(c)] \n",
    "\n",
    "    print(\"############# n = {}, c = {} #############\".format(n, c))\n",
    "    print(\"no watermark w/ chi2 testing: {:.8f}\".format(gaussMultTesting(tab_lists_no_wm, g_lists, m)))\n",
    "\n",
    "    tab_lists_wm_sing[0] = singleColumnWatermark(tab_lists_wm_sing[0], g_lists[0], m)\n",
    "    print(\"Single-column watermark w/ chi2 testing: {:.8f}\".format(gaussMultTesting(tab_lists_wm_sing, g_lists, m)))\n",
    "\n",
    "    for i, tab in enumerate(tab_lists_wm_all):\n",
    "        tab_lists_wm_all[i] = singleColumnWatermark(tab, g_lists[i], m)\n",
    "    print(\"All-column watermark w/ chi2 testing: {:.8f}\".format(gaussMultTesting(tab_lists_wm_all, g_lists, m)))\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "def singleColumnNoise(arr, noise_prop, std_prop):\n",
    "    \"\"\" add gaussian noise to a column\n",
    "    \"\"\"\n",
    "    arr_noise = arr.copy()\n",
    "    \n",
    "    num_rows_to_modify = int(len(arr) * noise_prop)\n",
    "    noise_std = np.sqrt(np.var(arr)) * std_prop\n",
    "    noise = np.random.normal(0, noise_std, num_rows_to_modify)\n",
    "    \n",
    "    # add noise to top x% rows\n",
    "    arr_noise[:num_rows_to_modify] += noise\n",
    "    \n",
    "    return arr_noise\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def get_optimal_col_and_m(X_num, delta=0.01):\n",
    "    '''  Empirical method to filter out poorly distributed numerical columns\n",
    "    '''\n",
    "    dataset  = X_num.copy()\n",
    "    \n",
    "    for col_id in range(dataset.shape[1]):\n",
    "        dataset[:, col_id] = (dataset[:, col_id] - np.mean(dataset[:, col_id])) / np.std(dataset[:, col_id])\n",
    "    \n",
    "    prop_min = 0.5 - delta\n",
    "    prop_max = 0.5 + delta\n",
    "    \n",
    "    m_list = list(range(1000, 5001, 500))\n",
    "    filter_arr = np.zeros(dataset.shape[1])\n",
    "    n = dataset.shape[0]\n",
    "    \n",
    "    test_times = 5\n",
    "    \n",
    "    for time in range(test_times):\n",
    "        print('test:', time)\n",
    "        for m in m_list:\n",
    "            print('m:', m)\n",
    "            unit_filter_arr = np.zeros(dataset.shape[1])\n",
    "            \n",
    "            for col_id in range(dataset.shape[1]):\n",
    "                arr = dataset[:, col_id]\n",
    "                col_GreenList = getGreenList(m=m)\n",
    "                prop_col = countElemInGreenList(arr, col_GreenList, m=m) / n\n",
    "                \n",
    "                if prop_col < prop_min or prop_col > prop_max:\n",
    "                    unit_filter_arr[col_id] += 1\n",
    "            \n",
    "            filter_arr = np.vstack((filter_arr, unit_filter_arr))\n",
    "    \n",
    "    filter_arr = filter_arr[1:, :]\n",
    "    filter_continuous = np.where(np.sum(filter_arr, axis=0)/(test_times*len(m_list)) < 0.1)[0]\n",
    "        \n",
    "    if not len(filter_continuous):\n",
    "        return np.array([]), None\n",
    "    \n",
    "    else:\n",
    "        filter_arr = filter_arr[:, filter_continuous]\n",
    "        print('filter_continuous:', filter_continuous, len(filter_continuous))\n",
    "        \n",
    "        agg_by_m = np.array([])\n",
    "\n",
    "        if len(filter_continuous):\n",
    "            \n",
    "            for step in range(len(m_list)):\n",
    "                agg_by_m = np.append(agg_by_m, filter_arr[step::len(m_list)].sum())\n",
    "            \n",
    "            print('pass_times', agg_by_m)\n",
    "            \n",
    "            optimal_m = m_list[np.argmin(agg_by_m)]\n",
    "        \n",
    "        else:\n",
    "            return filter_continuous, None\n",
    "        \n",
    "        print('optimal_m:', optimal_m)\n",
    "        \n",
    "    return filter_continuous, optimal_m\n",
    "\n",
    "def get_p_value(file_path, file_index):\n",
    "    ''' get p-value for (i) un-watermarked generated data; (ii) watermarked generated data; (iii) watermarked generated data with noise.\n",
    "        There should be \n",
    "        1 file GreenListsInfo.pkl documented greenlist informatiom\n",
    "        3 corresponding files (i) X_num_train_raw_syn_{file_index}.npy (ii)X_num_train_marked_syn_{file_index}.npy and (iii) X_num_train_noised_syn_{file_index}.npy under the file_path.\n",
    "    '''\n",
    "\n",
    "    sub_dir_path = Path(file_path)\n",
    "    with open(os.path.join(sub_dir_path, f'GreenListsInfo.pkl'), 'rb') as f:\n",
    "        GreenListsInfo = pickle.load(f)\n",
    "\n",
    "    continuous_indexes = GreenListsInfo['filter_continuous']\n",
    "    GreenLists = [GreenListsInfo['GreenLists'][i] for i in continuous_indexes]\n",
    "    col_mean = np.array(GreenListsInfo['col_mean'])\n",
    "    col_std = np.array(GreenListsInfo['col_std'])\n",
    "    \n",
    "    \n",
    "    raw_data = np.load(sub_dir_path / f'X_num_train_raw_syn_{file_index}.npy')\n",
    "    raw_data = (raw_data - col_mean) / col_std\n",
    "    raw_data = raw_data[:, continuous_indexes]\n",
    "    p_raw = maxGaussTesting(raw_data.T, GreenLists, m=len(GreenLists[0])*2)\n",
    "    \n",
    "    watermarked_data = np.load(sub_dir_path / f'X_num_train_marked_syn_{file_index}.npy')\n",
    "    watermarked_data = (watermarked_data - col_mean) / col_std\n",
    "    watermarked_data = watermarked_data[:, continuous_indexes]\n",
    "    p_watermarked = maxGaussTesting(watermarked_data.T, GreenLists, m=len(GreenLists[0])*2)\n",
    "\n",
    "    noised_data = np.load(sub_dir_path / f'X_num_train_noised_syn_{file_index}.npy')\n",
    "    noised_data = (noised_data - col_mean) / col_std\n",
    "    noised_data = noised_data[:, continuous_indexes]\n",
    "    p_noised = maxGaussTesting(noised_data.T, GreenLists, m=len(GreenLists[0])*2)\n",
    "    return [p_raw, p_watermarked, p_noised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 0\n",
      "m: 1000\n",
      "m: 1500\n",
      "m: 2000\n",
      "m: 2500\n",
      "m: 3000\n",
      "m: 3500\n",
      "m: 4000\n",
      "m: 4500\n",
      "m: 5000\n",
      "test: 1\n",
      "m: 1000\n",
      "m: 1500\n",
      "m: 2000\n",
      "m: 2500\n",
      "m: 3000\n",
      "m: 3500\n",
      "m: 4000\n",
      "m: 4500\n",
      "m: 5000\n",
      "test: 2\n",
      "m: 1000\n",
      "m: 1500\n",
      "m: 2000\n",
      "m: 2500\n",
      "m: 3000\n",
      "m: 3500\n",
      "m: 4000\n",
      "m: 4500\n",
      "m: 5000\n",
      "test: 3\n",
      "m: 1000\n",
      "m: 1500\n",
      "m: 2000\n",
      "m: 2500\n",
      "m: 3000\n",
      "m: 3500\n",
      "m: 4000\n",
      "m: 4500\n",
      "m: 5000\n",
      "test: 4\n",
      "m: 1000\n",
      "m: 1500\n",
      "m: 2000\n",
      "m: 2500\n",
      "m: 3000\n",
      "m: 3500\n",
      "m: 4000\n",
      "m: 4500\n",
      "m: 5000\n",
      "filter_continuous: [0 2 3 4 5] 5\n",
      "pass_times [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "optimal_m: 1000\n",
      "original_features: 8\n",
      "good_features: 5\n",
      "GreenList is successfully generated and saved.\n",
      "0\n",
      "-------------------------\n",
      "Finished processing task: california\n",
      "original auc 1.0\n",
      "noised auc 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# experiment codes\n",
    "# organize your folder as\n",
    "# base_path / task / X_num_train_raw_syn_{i}.npy\n",
    "\n",
    "\n",
    "\n",
    "GreenLists = []\n",
    "base_path = Path(r'C:\\watermark_exp_test')\n",
    "num_files = 10 # Total number of raw data files generated by the model\n",
    "num_sample_data = 5 # Number of sample data files used to estimate empirical distribution\n",
    "\n",
    "# Ensure your folder is organized as: base_path / task / X_num_train_raw_syn_{i}.npy\n",
    "\n",
    "for task in ['california']:\n",
    "             #, 'diabetes', 'gesture', 'house', 'wilt', 'higgs-small', 'miniboone']:\n",
    "    \n",
    "    sub_dir_path = base_path / task\n",
    "    \n",
    "    GreenLists = []\n",
    "    GreenListsInfo = {'GreenLists':[], 'col_mean':[], 'col_std':[], 'filter_continuous':[], 'm':[]}\n",
    "    \n",
    "    # Step 1: Read all raw generated data files\n",
    "    X = []\n",
    "    for i in range(num_files):\n",
    "        X.append(np.load(os.path.join(sub_dir_path, f'X_num_train_raw_syn_{i}.npy')))\n",
    "    \n",
    "    # Step 2: Combine sample data files to form an empirical distribution\n",
    "    # Use the first `num_sample_data` files for this purpose\n",
    "    X_arr = []\n",
    "    for i in range(num_sample_data):\n",
    "        X_arr.append(X[i])\n",
    "    X_num = np.concatenate(X_arr, axis=0)\n",
    "\n",
    "    # Step 3: Identify valid columns and the optimal value of `m`\n",
    "    filter_continuous, m = get_optimal_col_and_m(X_num)\n",
    "    \n",
    "    if len(filter_continuous):\n",
    "        # Step 4: Generate green lists and compute statistics for filtered columns\n",
    "        for col_id in range(X_num.shape[1]):\n",
    "            col_GreenList = getGreenList(m=m)\n",
    "            GreenLists.append(col_GreenList)\n",
    "            col_mean, col_std = np.mean(X_num[:, col_id]), np.std(X_num[:, col_id])\n",
    "            GreenListsInfo['col_mean'].append(col_mean)\n",
    "            GreenListsInfo['col_std'].append(col_std)\n",
    "\n",
    "        # Store green list information \n",
    "        GreenListsInfo['GreenLists'] = GreenLists\n",
    "        GreenListsInfo['filter_continuous'] = filter_continuous\n",
    "        GreenListsInfo['m'] = m\n",
    "\n",
    "        print('original_features:', X_num.shape[1])\n",
    "        print('good_features:', len(filter_continuous))\n",
    "        \n",
    "        with open(os.path.join(sub_dir_path, f'GreenListsInfo.pkl'), 'wb') as f:\n",
    "            pickle.dump(GreenListsInfo, f)\n",
    "        print('GreenList is successfully generated and saved.')\n",
    "\n",
    "        # Step 5: Apply watermarks and save modified files\n",
    "        for i in range(num_files):\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            X_num = X[i]\n",
    "            #np.save(os.path.join(sub_dir_path, str(i), f'X_num_train_raw_syn_{i}.npy'), X_num)\n",
    "            \n",
    "            for col_id in range(X_num.shape[1]):\n",
    "                col_arr = X_num[:, col_id]\n",
    "                col_arr_wm, col_mean, col_std = singleColumnWatermark(X_num[:, col_id], GreenListsInfo, col_id, m=m)\n",
    "                X_num[:, col_id] = col_arr_wm\n",
    "\n",
    "            np.save(os.path.join(sub_dir_path, f'X_num_train_marked_syn_{i}.npy'), X_num)\n",
    "\n",
    "            # Add Gaussian noise to each column\n",
    "            for col_id in range(X_num.shape[1]):\n",
    "                col_arr = X_num[:, col_id]\n",
    "                col_arr_noise = singleColumnNoise(col_arr, 0.95, 0.01)\n",
    "                X_num[:, col_id] = col_arr_noise\n",
    "\n",
    "            np.save(os.path.join(sub_dir_path, f'X_num_train_noised_syn_{i}.npy'), X_num)\n",
    "\n",
    "\n",
    "        print('-------------------------')\n",
    "        print(f'Finished processing task: {task}')\n",
    "\n",
    "        # Step 6: Compute AUC scores\n",
    "\n",
    "        p_raw_list = []\n",
    "        p_wm_list = []\n",
    "        p_noised_list = []\n",
    "\n",
    "        y = np.concatenate([np.ones(num_files), np.zeros(num_files)])\n",
    "        for i in range(num_files):\n",
    "            #print(filter_continuous)\n",
    "            p_raw, p_watermarked, p_noised = get_p_value(file_path = sub_dir_path, file_index = i)\n",
    "            p_raw_list.append(1 - p_raw)\n",
    "            p_wm_list.append(1 - p_watermarked)\n",
    "            p_noised_list.append(1 - p_noised)\n",
    "\n",
    "        print('original auc', roc_auc_score(y, np.array(p_wm_list + p_raw_list)))\n",
    "        print('noised auc', roc_auc_score(y, np.array(p_noised_list + p_raw_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "california\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.022215891410705662\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.000268862759779327\n",
      "wasserstein_distance between watermarked data and noised data: 0.0019194950523449125\n",
      "-----------------------\n",
      "gesture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.060179562425967205\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.0002658256081741967\n",
      "wasserstein_distance between watermarked data and noised data: 0.0018698861508541851\n",
      "-----------------------\n",
      "house\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.03145603665798752\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.0003772485544704065\n",
      "wasserstein_distance between watermarked data and noised data: 0.0025162109292464385\n",
      "-----------------------\n",
      "wilt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.07672873875226098\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.00044484284926277673\n",
      "wasserstein_distance between watermarked data and noised data: 0.002184707995364442\n",
      "-----------------------\n",
      "higgs-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:33<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.014216595430518914\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.0003442360305712603\n",
      "wasserstein_distance between watermarked data and noised data: 0.0017334297816506618\n",
      "-----------------------\n",
      "miniboone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:07<00:00,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasserstein_distance between original data and synthetic data: 0.01610096401723159\n",
      "wasserstein_distance between synthetic data and watermarked data: 0.00014663095409377783\n",
      "wasserstein_distance between watermarked data and noised data: 0.003926176236255854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import wasserstein_distance\n",
    "from tqdm import tqdm\n",
    "##### adding watermark #####\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import binom\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# data_path saving the original data\n",
    "data_path = Path(r'C:\\data')\n",
    "# base_path saving generated data (watermarked / unwatermarked / noised)\n",
    "base_path = Path(r'C:\\watermark_exp')\n",
    "num_files = 50\n",
    "\n",
    "def normalize_array(x):\n",
    "    for col_id in range(x.shape[1]):\n",
    "        x[:, col_id] = (x[:, col_id] - np.mean(x[:, col_id])) / np.std(x[:, col_id])\n",
    "\n",
    "for task in ['california', 'gesture', 'house', 'wilt', 'higgs-small', 'miniboone']:\n",
    "    print('-----------------------')\n",
    "    print(task)\n",
    "    original = np.load(data_path / task / 'X_num_train.npy')\n",
    "    distance_raw_ori = []\n",
    "    distance_marked_raw = []\n",
    "    distance_noised_marked = []\n",
    "    \n",
    "    for i in tqdm(num_files):\n",
    "        raw_syn = np.load(base_path / task / f'X_num_train_raw_syn_{i}.npy')\n",
    "        marked_syn = np.load(base_path / task / f'X_num_train_marked_syn_{i}.npy')\n",
    "        noised_syn = np.load(base_path / task / f'X_num_train_noised_syn_{i}.npy')\n",
    "\n",
    "        normalize_array(original)\n",
    "        normalize_array(raw_syn)\n",
    "        normalize_array(marked_syn)\n",
    "        normalize_array(noised_syn)\n",
    "\n",
    "        unit_distance_raw_ori = []\n",
    "        unit_distance_marked_raw = []\n",
    "        unit_distance_noised_marked = []\n",
    "\n",
    "        for col_id in range(original.shape[1]):\n",
    "            unit_distance_raw_ori.append(wasserstein_distance(original[:,col_id], raw_syn[:,col_id]))\n",
    "            unit_distance_marked_raw.append(wasserstein_distance(marked_syn[:,col_id], raw_syn[:,col_id]))\n",
    "            unit_distance_noised_marked.append(wasserstein_distance(marked_syn[:,col_id], noised_syn[:,col_id]))\n",
    "            \n",
    "        distance_raw_ori.append(sum(unit_distance_raw_ori) / len(unit_distance_raw_ori))\n",
    "        distance_marked_raw.append(sum(unit_distance_marked_raw) / len(unit_distance_marked_raw))\n",
    "        distance_noised_marked.append(sum(unit_distance_noised_marked) / len(unit_distance_noised_marked))\n",
    "    print('wasserstein_distance between original data and synthetic data:', sum(distance_raw_ori)/len(distance_raw_ori))\n",
    "    print('wasserstein_distance between synthetic data and watermarked data:', sum(distance_marked_raw)/len(distance_marked_raw))\n",
    "    print('wasserstein_distance between watermarked data and noised data:', sum(distance_noised_marked)/len(distance_noised_marked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
